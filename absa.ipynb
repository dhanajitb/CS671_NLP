{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the imports are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "\n",
    "import onmt\n",
    "import onmt.io\n",
    "import onmt.Models\n",
    "import onmt.ModelConstructor\n",
    "import onmt.modules\n",
    "from onmt.Utils import use_gpu\n",
    "import opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='train.py',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='train.py',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# opts.py\n",
    "opts.add_md_help_argument(parser)\n",
    "opts.model_opts(parser)\n",
    "opts.train_opts(parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = parser.parse_args()\n",
    "opt = parser.parse_args([\"-data\", \"data/multi30k.atok.low\", \"-save_model\", \"multi30k_model\", \"-gpuid\", \"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTM'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.rnn_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.word_vec_size != -1:\n",
    "    opt.src_word_vec_size = opt.word_vec_size\n",
    "    opt.tgt_word_vec_size = opt.word_vec_size\n",
    "\n",
    "if opt.layers != -1:\n",
    "    opt.enc_layers = opt.layers\n",
    "    opt.dec_layers = opt.layers\n",
    "\n",
    "opt.brnn = (opt.encoder_type == \"brnn\")\n",
    "if opt.seed > 0:\n",
    "    random.seed(opt.seed)\n",
    "    torch.manual_seed(opt.seed)\n",
    "\n",
    "if opt.rnn_type == \"SRU\" and not opt.gpuid:\n",
    "    raise AssertionError(\"Using SRU requires -gpuid set.\")\n",
    "\n",
    "if torch.cuda.is_available() and not opt.gpuid:\n",
    "    print(\"WARNING: You have a CUDA device, should run with -gpuid 0\")\n",
    "\n",
    "if opt.gpuid:\n",
    "    cuda.set_device(opt.gpuid[0])\n",
    "    if opt.seed > 0:\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "\n",
    "if len(opt.gpuid) > 1:\n",
    "    sys.stderr.write(\"Sorry, multigpu isn't supported yet, coming soon!\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Set up the Crayon logging server.\n",
    "if opt.exp_host != \"\":\n",
    "    from pycrayon import CrayonClient\n",
    "\n",
    "    cc = CrayonClient(hostname=opt.exp_host)\n",
    "\n",
    "    experiments = cc.get_experiment_names()\n",
    "    print(experiments)\n",
    "    if opt.exp in experiments:\n",
    "        cc.remove_experiment(opt.exp)\n",
    "    experiment = cc.create_experiment(opt.exp)\n",
    "\n",
    "if opt.tensorboard:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    writer = SummaryWriter(opt.tensorboard_log_dir, comment=\"Onmt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the user-defined batch-level traing progress\n",
    "    report function.\n",
    "\n",
    "    Args:\n",
    "        epoch(int): current epoch count.\n",
    "        batch(int): current batch count.\n",
    "        num_batches(int): total number of batches.\n",
    "        start_time(float): last report time.\n",
    "        lr(float): current learning rate.\n",
    "        report_stats(Statistics): old Statistics instance.\n",
    "    Returns:\n",
    "        report_stats(Statistics): updated Statistics instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_func(epoch, batch, num_batches, start_time, lr, report_stats):\n",
    "    if batch % opt.report_every == -1 % opt.report_every:\n",
    "        report_stats.output(epoch, batch + 1, num_batches, start_time)\n",
    "        if opt.exp_host:\n",
    "            report_stats.log(\"progress\", experiment, lr)\n",
    "        if opt.tensorboard:\n",
    "            report_stats.log_tensorboard(\"progress\", writer, lr, epoch)\n",
    "        report_stats = onmt.Statistics()\n",
    "\n",
    "    return report_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \"\"\" An Ordered Dataset Iterator, supporting multiple datasets,\n",
    "        and lazy loading.\n",
    "\n",
    "    Args:\n",
    "        datsets (list): a list of datasets, which are lazily loaded.\n",
    "        fields (dict): fields dict for the datasets.\n",
    "        batch_size (int): batch size.\n",
    "        batch_size_fn: custom batch process function.\n",
    "        device: the GPU device.\n",
    "        is_train (bool): train or valid?\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLazyIter(object):\n",
    "    def __init__(self, datasets, fields, batch_size, batch_size_fn,\n",
    "                 device, is_train):\n",
    "        self.datasets = datasets\n",
    "        self.fields = fields\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_fn = batch_size_fn\n",
    "        self.device = device\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.cur_iter = self._next_dataset_iterator(datasets)\n",
    "        # We have at least one dataset.\n",
    "        assert self.cur_iter is not None\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset_iter = (d for d in self.datasets)\n",
    "        while self.cur_iter is not None:\n",
    "            for batch in self.cur_iter:\n",
    "                yield batch\n",
    "            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        # We return the len of cur_dataset, otherwise we need to load\n",
    "        # all datasets to determine the real len, which loses the benefit\n",
    "        # of lazy loading.\n",
    "        assert self.cur_iter is not None\n",
    "        return len(self.cur_iter)\n",
    "\n",
    "    def get_cur_dataset(self):\n",
    "        return self.cur_dataset\n",
    "\n",
    "    def _next_dataset_iterator(self, dataset_iter):\n",
    "        try:\n",
    "            self.cur_dataset = next(dataset_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        # We clear `fields` when saving, restore when loading.\n",
    "        self.cur_dataset.fields = self.fields\n",
    "\n",
    "        # Sort batch by decreasing lengths of sentence required by pytorch.\n",
    "        # sort=False means \"Use dataset's sortkey instead of iterator's\".\n",
    "        return onmt.io.OrderedIterator(\n",
    "            dataset=self.cur_dataset, batch_size=self.batch_size,\n",
    "            batch_size_fn=self.batch_size_fn,\n",
    "            device=self.device, train=self.is_train,\n",
    "            sort=False, sort_within_batch=True,\n",
    "            repeat=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This returns user-defined train/validate data iterator for the trainer\n",
    "    to iterate over during each train epoch. We implement simple\n",
    "    ordered iterator strategy here, but more sophisticated strategy\n",
    "    like curriculum learning is ok too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_iter(datasets, fields, opt, is_train=True):\n",
    "\n",
    "    batch_size = opt.batch_size if is_train else opt.valid_batch_size\n",
    "    batch_size_fn = None\n",
    "    if is_train and opt.batch_type == \"tokens\":\n",
    "        def batch_size_fn(new, count, sofar):\n",
    "            return sofar + max(len(new.tgt), len(new.src)) + 1\n",
    "\n",
    "    device = opt.gpuid[0] if opt.gpuid else -1\n",
    "\n",
    "    return DatasetLazyIter(datasets, fields, batch_size, batch_size_fn,\n",
    "                           device, is_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This returns user-defined LossCompute object, which is used to\n",
    "    compute loss in train/validate process. You can implement your\n",
    "    own *LossCompute class, by subclassing LossComputeBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_compute(model, tgt_vocab, opt):\n",
    "\n",
    "    if opt.copy_attn:\n",
    "        compute = onmt.modules.CopyGeneratorLossCompute(\n",
    "            model.generator, tgt_vocab, opt.copy_attn_force,\n",
    "            opt.copy_loss_by_seqlength)\n",
    "    else:\n",
    "        compute = onmt.Loss.NMTLossCompute(\n",
    "            model.generator, tgt_vocab,\n",
    "            label_smoothing=opt.label_smoothing)\n",
    "\n",
    "    if use_gpu(opt):\n",
    "        compute.cuda()\n",
    "\n",
    "    return compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, fields, optim, data_type, model_opt):\n",
    "    train_loss = make_loss_compute(model, fields[\"tgt\"].vocab, opt)\n",
    "    valid_loss = make_loss_compute(model, fields[\"tgt\"].vocab, opt)\n",
    "\n",
    "    trunc_size = opt.truncated_decoder  # Badly named...\n",
    "    shard_size = opt.max_generator_batches\n",
    "    norm_method = opt.normalization\n",
    "    grad_accum_count = opt.accum_count\n",
    "\n",
    "    trainer = onmt.Trainer(model, train_loss, valid_loss, optim,\n",
    "                           trunc_size, shard_size, data_type,\n",
    "                           norm_method, grad_accum_count)\n",
    "\n",
    "    print('\\nStart training...')\n",
    "    print(' * number of epochs: %d, starting from Epoch %d' %\n",
    "          (opt.epochs + 1 - opt.start_epoch, opt.start_epoch))\n",
    "    print(' * batch size: %d' % opt.batch_size)\n",
    "\n",
    "    for epoch in range(opt.start_epoch, opt.epochs + 1):\n",
    "        print('')\n",
    "\n",
    "        # 1. Train for one epoch on the training set.\n",
    "        train_iter = make_dataset_iter(lazily_load_dataset(\"train\"),\n",
    "                                       fields, opt)\n",
    "        train_stats = trainer.train(train_iter, epoch, report_func)\n",
    "        print('Train perplexity: %g' % train_stats.ppl())\n",
    "        print('Train accuracy: %g' % train_stats.accuracy())\n",
    "\n",
    "        # 2. Validate on the validation set.\n",
    "        valid_iter = make_dataset_iter(lazily_load_dataset(\"valid\"),\n",
    "                                       fields, opt,\n",
    "                                       is_train=False)\n",
    "        valid_stats = trainer.validate(valid_iter)\n",
    "        print('Validation perplexity: %g' % valid_stats.ppl())\n",
    "        print('Validation accuracy: %g' % valid_stats.accuracy())\n",
    "\n",
    "        # 3. Log to remote server.\n",
    "        if opt.exp_host:\n",
    "            train_stats.log(\"train\", experiment, optim.lr)\n",
    "            valid_stats.log(\"valid\", experiment, optim.lr)\n",
    "        if opt.tensorboard:\n",
    "            train_stats.log_tensorboard(\"train\", writer, optim.lr, epoch)\n",
    "            train_stats.log_tensorboard(\"valid\", writer, optim.lr, epoch)\n",
    "\n",
    "        # 4. Update the learning rate\n",
    "        trainer.epoch_step(valid_stats.ppl(), epoch)\n",
    "\n",
    "        # 5. Drop a checkpoint if needed.\n",
    "        if epoch >= opt.start_checkpoint_at:\n",
    "            trainer.drop_checkpoint(model_opt, epoch, fields, valid_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_save_model_path():\n",
    "    save_model_path = os.path.abspath(opt.save_model)\n",
    "    model_dirname = os.path.dirname(save_model_path)\n",
    "    if not os.path.exists(model_dirname):\n",
    "        os.makedirs(model_dirname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    print('* number of parameters: %d' % n_params)\n",
    "    enc = 0\n",
    "    dec = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'encoder' in name:\n",
    "            enc += param.nelement()\n",
    "        elif 'decoder' or 'generator' in name:\n",
    "            dec += param.nelement()\n",
    "    print('encoder: ', enc)\n",
    "    print('decoder: ', dec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset generator. Don't do extra stuff here, like printing,\n",
    "    because they will be postponed to the first loading time.\n",
    "\n",
    "    Args:\n",
    "        corpus_type: 'train' or 'valid'\n",
    "    Returns:\n",
    "        A list of dataset, the dataset(s) are lazily loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazily_load_dataset(corpus_type):\n",
    "\n",
    "    assert corpus_type in [\"train\", \"valid\"]\n",
    "\n",
    "    def lazy_dataset_loader(pt_file, corpus_type):\n",
    "        dataset = torch.load(pt_file)\n",
    "        print('Loading %s dataset from %s, number of examples: %d' %\n",
    "              (corpus_type, pt_file, len(dataset)))\n",
    "        return dataset\n",
    "\n",
    "    # Sort the glob output by file name (by increasing indexes).\n",
    "    pts = sorted(glob.glob(opt.data + '.' + corpus_type + '.[0-9]*.pt'))\n",
    "    if pts:\n",
    "        for pt in pts:\n",
    "            yield lazy_dataset_loader(pt, corpus_type)\n",
    "    else:\n",
    "        # Only one onmt.io.*Dataset, simple!\n",
    "        pt = opt.data + '.' + corpus_type + '.pt'\n",
    "        yield lazy_dataset_loader(pt, corpus_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fields(dataset, data_type, checkpoint):\n",
    "    if checkpoint is not None:\n",
    "        print('Loading vocab from checkpoint at %s.' % opt.train_from)\n",
    "        fields = onmt.io.load_fields_from_vocab(\n",
    "            checkpoint['vocab'], data_type)\n",
    "    else:\n",
    "        fields = onmt.io.load_fields_from_vocab(\n",
    "            torch.load(opt.data + '.vocab.pt'), data_type)\n",
    "    fields = dict([(k, f) for (k, f) in fields.items()\n",
    "                   if k in dataset.examples[0].__dict__])\n",
    "\n",
    "    if data_type == 'text':\n",
    "        print(' * vocabulary size. source = %d; target = %d' %\n",
    "              (len(fields['src'].vocab), len(fields['tgt'].vocab)))\n",
    "    else:\n",
    "        print(' * vocabulary size. target = %d' %\n",
    "              (len(fields['tgt'].vocab)))\n",
    "\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_report_features(fields):\n",
    "    src_features = onmt.io.collect_features(fields, side='src')\n",
    "    tgt_features = onmt.io.collect_features(fields, side='tgt')\n",
    "\n",
    "    for j, feat in enumerate(src_features):\n",
    "        print(' * src feature %d size = %d' % (j, len(fields[feat].vocab)))\n",
    "    for j, feat in enumerate(tgt_features):\n",
    "        print(' * tgt feature %d size = %d' % (j, len(fields[feat].vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_opt, opt, fields, checkpoint):\n",
    "    print('Building model...')\n",
    "    model = onmt.ModelConstructor.make_base_model(model_opt, fields,\n",
    "                                                  use_gpu(opt), checkpoint)\n",
    "    if len(opt.gpuid) > 1:\n",
    "        print('Multi gpu training: ', opt.gpuid)\n",
    "        model = nn.DataParallel(model, device_ids=opt.gpuid, dim=1)\n",
    "    print(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optim(model, checkpoint):\n",
    "    if opt.train_from:\n",
    "        print('Loading optimizer from checkpoint.')\n",
    "        optim = checkpoint['optim']\n",
    "        optim.optimizer.load_state_dict(\n",
    "            checkpoint['optim'].optimizer.state_dict())\n",
    "    else:\n",
    "        print('Making optimizer for training.')\n",
    "        optim = onmt.Optim(\n",
    "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
    "            lr_decay=opt.learning_rate_decay,\n",
    "            start_decay_at=opt.start_decay_at,\n",
    "            beta1=opt.adam_beta1,\n",
    "            beta2=opt.adam_beta2,\n",
    "            adagrad_accum=opt.adagrad_accumulator_init,\n",
    "            decay_method=opt.decay_method,\n",
    "            warmup_steps=opt.warmup_steps,\n",
    "            model_size=opt.rnn_size)\n",
    "\n",
    "    optim.set_parameters(model.parameters())\n",
    "\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load checkpoint if we resume from a previous training.\n",
    "    if opt.train_from:\n",
    "        print('Loading checkpoint from %s' % opt.train_from)\n",
    "        checkpoint = torch.load(opt.train_from,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "        model_opt = checkpoint['opt']\n",
    "        # I don't like reassigning attributes of opt: it's not clear.\n",
    "        opt.start_epoch = checkpoint['epoch'] + 1\n",
    "    else:\n",
    "        checkpoint = None\n",
    "        model_opt = opt\n",
    "\n",
    "    # Peek the fisrt dataset to determine the data_type.\n",
    "    # (All datasets have the same data_type).\n",
    "    first_dataset = next(lazily_load_dataset(\"train\"))\n",
    "    data_type = first_dataset.data_type\n",
    "\n",
    "    # Load fields generated from preprocess phase.\n",
    "    fields = load_fields(first_dataset, data_type, checkpoint)\n",
    "\n",
    "    # Report src/tgt features.\n",
    "    collect_report_features(fields)\n",
    "\n",
    "    # Build model.\n",
    "    model = build_model(model_opt, opt, fields, checkpoint)\n",
    "    tally_parameters(model)\n",
    "    check_save_model_path()\n",
    "\n",
    "    # Build optimizer.\n",
    "    optim = build_optim(model, checkpoint)\n",
    "\n",
    "    # Do training.\n",
    "    train_model(model, fields, optim, data_type, model_opt)\n",
    "\n",
    "    # If using tensorboard for logging, close the writer after training.\n",
    "    if opt.tensorboard:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      " * vocabulary size. source = 10841; target = 18563\n",
      "Building model...\n",
      "Intializing model parameters.\n",
      "NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(10841, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(18563, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "      (sm): Softmax()\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=18563, bias=True)\n",
      "    (1): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "* number of parameters: 33768063\n",
      "encoder:  9428500\n",
      "decoder:  24339563\n",
      "Making optimizer for training.\n",
      "\n",
      "Start training...\n",
      " * number of epochs: 13, starting from Epoch 1\n",
      " * batch size: 64\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dhanajit/OpenNMT-py/onmt/modules/GlobalAttention.py:177: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  align_vectors = self.sm(align.view(batch*targetL, sourceL))\n",
      "/users/phd/dhanajit/gpu2_dhanajit/anaconda3/envs/ABSA/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,    50/  454; acc:   7.77; ppl: 35027.49; 6198 src tok/s; 6445 tgt tok/s;      7 s elapsed\n",
      "Epoch  1,   100/  454; acc:   7.45; ppl: 6368.84; 6791 src tok/s; 7055 tgt tok/s;     13 s elapsed\n",
      "Epoch  1,   150/  454; acc:  10.45; ppl: 964.15; 6815 src tok/s; 7065 tgt tok/s;     19 s elapsed\n",
      "Epoch  1,   200/  454; acc:  17.10; ppl: 423.29; 6901 src tok/s; 7213 tgt tok/s;     25 s elapsed\n",
      "Epoch  1,   250/  454; acc:  19.80; ppl: 294.18; 6698 src tok/s; 6971 tgt tok/s;     31 s elapsed\n",
      "Epoch  1,   300/  454; acc:  21.31; ppl: 214.58; 6726 src tok/s; 6935 tgt tok/s;     38 s elapsed\n",
      "Epoch  1,   350/  454; acc:  22.98; ppl: 167.44; 6990 src tok/s; 7273 tgt tok/s;     44 s elapsed\n",
      "Epoch  1,   400/  454; acc:  24.35; ppl: 136.29; 6583 src tok/s; 6788 tgt tok/s;     50 s elapsed\n",
      "Epoch  1,   450/  454; acc:  25.74; ppl: 111.85; 6453 src tok/s; 6689 tgt tok/s;     57 s elapsed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/phd/dhanajit/gpu2_dhanajit/anaconda3/envs/ABSA/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 616.054\n",
      "Train accuracy: 17.537\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 95.0955\n",
      "Validation accuracy: 28.239\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  2,    50/  454; acc:  27.95; ppl:  91.67; 6607 src tok/s; 6871 tgt tok/s;      6 s elapsed\n",
      "Epoch  2,   100/  454; acc:  29.77; ppl:  78.39; 6558 src tok/s; 6813 tgt tok/s;     13 s elapsed\n",
      "Epoch  2,   150/  454; acc:  30.62; ppl:  71.57; 6423 src tok/s; 6659 tgt tok/s;     19 s elapsed\n",
      "Epoch  2,   200/  454; acc:  31.41; ppl:  64.82; 6697 src tok/s; 7000 tgt tok/s;     25 s elapsed\n",
      "Epoch  2,   250/  454; acc:  32.60; ppl:  58.15; 7286 src tok/s; 7583 tgt tok/s;     31 s elapsed\n",
      "Epoch  2,   300/  454; acc:  32.82; ppl:  56.65; 7278 src tok/s; 7505 tgt tok/s;     37 s elapsed\n",
      "Epoch  2,   350/  454; acc:  33.62; ppl:  51.78; 7111 src tok/s; 7398 tgt tok/s;     43 s elapsed\n",
      "Epoch  2,   400/  454; acc:  34.37; ppl:  49.58; 7295 src tok/s; 7522 tgt tok/s;     49 s elapsed\n",
      "Epoch  2,   450/  454; acc:  34.69; ppl:  47.32; 7204 src tok/s; 7467 tgt tok/s;     54 s elapsed\n",
      "Train perplexity: 61.6322\n",
      "Train accuracy: 32.0284\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 41.7052\n",
      "Validation accuracy: 36.6965\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  3,    50/  454; acc:  35.59; ppl:  43.37; 6627 src tok/s; 6892 tgt tok/s;      6 s elapsed\n",
      "Epoch  3,   100/  454; acc:  36.71; ppl:  40.56; 6411 src tok/s; 6660 tgt tok/s;     13 s elapsed\n",
      "Epoch  3,   150/  454; acc:  37.05; ppl:  38.62; 6422 src tok/s; 6658 tgt tok/s;     19 s elapsed\n",
      "Epoch  3,   200/  454; acc:  38.26; ppl:  35.31; 7220 src tok/s; 7546 tgt tok/s;     25 s elapsed\n",
      "Epoch  3,   250/  454; acc:  40.55; ppl:  31.47; 6724 src tok/s; 6998 tgt tok/s;     31 s elapsed\n",
      "Epoch  3,   300/  454; acc:  42.33; ppl:  29.07; 6951 src tok/s; 7168 tgt tok/s;     37 s elapsed\n",
      "Epoch  3,   350/  454; acc:  45.09; ppl:  25.10; 6549 src tok/s; 6813 tgt tok/s;     44 s elapsed\n",
      "Epoch  3,   400/  454; acc:  46.62; ppl:  22.94; 6486 src tok/s; 6688 tgt tok/s;     50 s elapsed\n",
      "Epoch  3,   450/  454; acc:  47.68; ppl:  20.75; 6797 src tok/s; 7045 tgt tok/s;     57 s elapsed\n",
      "Train perplexity: 30.7688\n",
      "Train accuracy: 41.1979\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 18.2091\n",
      "Validation accuracy: 49.8226\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  4,    50/  454; acc:  50.05; ppl:  17.79; 6691 src tok/s; 6957 tgt tok/s;      6 s elapsed\n",
      "Epoch  4,   100/  454; acc:  51.00; ppl:  16.80; 7194 src tok/s; 7474 tgt tok/s;     12 s elapsed\n",
      "Epoch  4,   150/  454; acc:  52.42; ppl:  15.54; 7125 src tok/s; 7387 tgt tok/s;     18 s elapsed\n",
      "Epoch  4,   200/  454; acc:  53.35; ppl:  14.04; 7340 src tok/s; 7671 tgt tok/s;     24 s elapsed\n",
      "Epoch  4,   250/  454; acc:  54.39; ppl:  13.44; 7254 src tok/s; 7549 tgt tok/s;     29 s elapsed\n",
      "Epoch  4,   300/  454; acc:  54.24; ppl:  13.30; 6594 src tok/s; 6799 tgt tok/s;     36 s elapsed\n",
      "Epoch  4,   350/  454; acc:  55.55; ppl:  12.33; 6576 src tok/s; 6842 tgt tok/s;     42 s elapsed\n",
      "Epoch  4,   400/  454; acc:  56.09; ppl:  12.00; 6645 src tok/s; 6852 tgt tok/s;     49 s elapsed\n",
      "Epoch  4,   450/  454; acc:  56.49; ppl:  11.51; 6847 src tok/s; 7097 tgt tok/s;     55 s elapsed\n",
      "Train perplexity: 13.8782\n",
      "Train accuracy: 53.7879\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 11.6708\n",
      "Validation accuracy: 57.5635\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  5,    50/  454; acc:  57.85; ppl:  10.51; 6429 src tok/s; 6686 tgt tok/s;      7 s elapsed\n",
      "Epoch  5,   100/  454; acc:  58.25; ppl:  10.29; 6390 src tok/s; 6638 tgt tok/s;     13 s elapsed\n",
      "Epoch  5,   150/  454; acc:  58.64; ppl:   9.79; 6653 src tok/s; 6897 tgt tok/s;     19 s elapsed\n",
      "Epoch  5,   200/  454; acc:  59.41; ppl:   9.18; 6583 src tok/s; 6880 tgt tok/s;     26 s elapsed\n",
      "Epoch  5,   250/  454; acc:  60.30; ppl:   8.86; 6973 src tok/s; 7257 tgt tok/s;     32 s elapsed\n",
      "Epoch  5,   300/  454; acc:  59.35; ppl:   9.15; 6710 src tok/s; 6919 tgt tok/s;     38 s elapsed\n",
      "Epoch  5,   350/  454; acc:  60.75; ppl:   8.56; 6450 src tok/s; 6711 tgt tok/s;     45 s elapsed\n",
      "Epoch  5,   400/  454; acc:  60.54; ppl:   8.53; 6746 src tok/s; 6955 tgt tok/s;     51 s elapsed\n",
      "Epoch  5,   450/  454; acc:  60.50; ppl:   8.42; 6480 src tok/s; 6717 tgt tok/s;     57 s elapsed\n",
      "Train perplexity: 9.19497\n",
      "Train accuracy: 59.5542\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 9.30066\n",
      "Validation accuracy: 61.3736\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  6,    50/  454; acc:  61.74; ppl:   7.79; 6843 src tok/s; 7116 tgt tok/s;      6 s elapsed\n",
      "Epoch  6,   100/  454; acc:  61.76; ppl:   7.71; 6561 src tok/s; 6816 tgt tok/s;     13 s elapsed\n",
      "Epoch  6,   150/  454; acc:  62.46; ppl:   7.39; 6466 src tok/s; 6704 tgt tok/s;     19 s elapsed\n",
      "Epoch  6,   200/  454; acc:  63.08; ppl:   6.99; 6684 src tok/s; 6986 tgt tok/s;     25 s elapsed\n",
      "Epoch  6,   250/  454; acc:  63.61; ppl:   6.82; 6519 src tok/s; 6784 tgt tok/s;     31 s elapsed\n",
      "Epoch  6,   300/  454; acc:  62.52; ppl:   7.12; 6771 src tok/s; 6982 tgt tok/s;     38 s elapsed\n",
      "Epoch  6,   350/  454; acc:  63.60; ppl:   6.75; 6602 src tok/s; 6869 tgt tok/s;     44 s elapsed\n",
      "Epoch  6,   400/  454; acc:  63.46; ppl:   6.85; 6838 src tok/s; 7050 tgt tok/s;     50 s elapsed\n",
      "Epoch  6,   450/  454; acc:  63.21; ppl:   6.69; 6844 src tok/s; 7093 tgt tok/s;     56 s elapsed\n",
      "Train perplexity: 7.08951\n",
      "Train accuracy: 62.8664\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 8.3148\n",
      "Validation accuracy: 62.8991\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  7,    50/  454; acc:  64.16; ppl:   6.34; 6729 src tok/s; 6997 tgt tok/s;      6 s elapsed\n",
      "Epoch  7,   100/  454; acc:  64.58; ppl:   6.18; 6930 src tok/s; 7199 tgt tok/s;     12 s elapsed\n",
      "Epoch  7,   150/  454; acc:  64.73; ppl:   6.03; 6853 src tok/s; 7105 tgt tok/s;     18 s elapsed\n",
      "Epoch  7,   200/  454; acc:  65.56; ppl:   5.69; 6658 src tok/s; 6959 tgt tok/s;     25 s elapsed\n",
      "Epoch  7,   250/  454; acc:  65.92; ppl:   5.64; 6683 src tok/s; 6955 tgt tok/s;     31 s elapsed\n",
      "Epoch  7,   300/  454; acc:  64.87; ppl:   5.90; 6678 src tok/s; 6886 tgt tok/s;     37 s elapsed\n",
      "Epoch  7,   350/  454; acc:  65.75; ppl:   5.61; 6662 src tok/s; 6932 tgt tok/s;     44 s elapsed\n",
      "Epoch  7,   400/  454; acc:  65.48; ppl:   5.71; 6786 src tok/s; 6998 tgt tok/s;     50 s elapsed\n",
      "Epoch  7,   450/  454; acc:  65.35; ppl:   5.61; 6415 src tok/s; 6649 tgt tok/s;     56 s elapsed\n",
      "Train perplexity: 5.83482\n",
      "Train accuracy: 65.1926\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 7.93153\n",
      "Validation accuracy: 63.8428\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  8,    50/  454; acc:  66.58; ppl:   5.32; 7036 src tok/s; 7317 tgt tok/s;      6 s elapsed\n",
      "Epoch  8,   100/  454; acc:  66.83; ppl:   5.19; 7140 src tok/s; 7417 tgt tok/s;     12 s elapsed\n",
      "Epoch  8,   150/  454; acc:  66.74; ppl:   5.11; 7249 src tok/s; 7516 tgt tok/s;     18 s elapsed\n",
      "Epoch  8,   200/  454; acc:  67.54; ppl:   4.87; 7170 src tok/s; 7494 tgt tok/s;     23 s elapsed\n",
      "Epoch  8,   250/  454; acc:  67.66; ppl:   4.86; 7317 src tok/s; 7615 tgt tok/s;     29 s elapsed\n",
      "Epoch  8,   300/  454; acc:  66.71; ppl:   5.04; 7219 src tok/s; 7444 tgt tok/s;     35 s elapsed\n",
      "Epoch  8,   350/  454; acc:  67.64; ppl:   4.83; 7125 src tok/s; 7413 tgt tok/s;     41 s elapsed\n",
      "Epoch  8,   400/  454; acc:  67.23; ppl:   4.90; 7200 src tok/s; 7424 tgt tok/s;     47 s elapsed\n",
      "Epoch  8,   450/  454; acc:  67.25; ppl:   4.82; 7199 src tok/s; 7461 tgt tok/s;     53 s elapsed\n",
      "Train perplexity: 4.97776\n",
      "Train accuracy: 67.164\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation perplexity: 7.54753\n",
      "Validation accuracy: 65.0419\n",
      "Decaying learning rate to 0.5\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch  9,    50/  454; acc:  69.31; ppl:   4.39; 6938 src tok/s; 7214 tgt tok/s;      6 s elapsed\n",
      "Epoch  9,   100/  454; acc:  70.58; ppl:   4.11; 6680 src tok/s; 6940 tgt tok/s;     12 s elapsed\n",
      "Epoch  9,   150/  454; acc:  70.71; ppl:   4.01; 6488 src tok/s; 6727 tgt tok/s;     19 s elapsed\n",
      "Epoch  9,   200/  454; acc:  71.73; ppl:   3.76; 6561 src tok/s; 6857 tgt tok/s;     25 s elapsed\n",
      "Epoch  9,   250/  454; acc:  72.06; ppl:   3.69; 6787 src tok/s; 7064 tgt tok/s;     31 s elapsed\n",
      "Epoch  9,   300/  454; acc:  71.21; ppl:   3.82; 6662 src tok/s; 6870 tgt tok/s;     38 s elapsed\n",
      "Epoch  9,   350/  454; acc:  72.13; ppl:   3.60; 6537 src tok/s; 6802 tgt tok/s;     44 s elapsed\n",
      "Epoch  9,   400/  454; acc:  72.27; ppl:   3.60; 6703 src tok/s; 6912 tgt tok/s;     50 s elapsed\n",
      "Epoch  9,   450/  454; acc:  72.80; ppl:   3.49; 7046 src tok/s; 7303 tgt tok/s;     56 s elapsed\n",
      "Train perplexity: 3.80769\n",
      "Train accuracy: 71.4731\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 6.70814\n",
      "Validation accuracy: 67.44\n",
      "Decaying learning rate to 0.25\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch 10,    50/  454; acc:  72.04; ppl:   3.66; 6746 src tok/s; 7015 tgt tok/s;      6 s elapsed\n",
      "Epoch 10,   100/  454; acc:  73.13; ppl:   3.45; 6440 src tok/s; 6690 tgt tok/s;     13 s elapsed\n",
      "Epoch 10,   150/  454; acc:  73.31; ppl:   3.38; 6576 src tok/s; 6818 tgt tok/s;     19 s elapsed\n",
      "Epoch 10,   200/  454; acc:  74.36; ppl:   3.20; 6540 src tok/s; 6835 tgt tok/s;     25 s elapsed\n",
      "Epoch 10,   250/  454; acc:  74.85; ppl:   3.13; 6645 src tok/s; 6915 tgt tok/s;     32 s elapsed\n",
      "Epoch 10,   300/  454; acc:  74.04; ppl:   3.21; 6641 src tok/s; 6848 tgt tok/s;     38 s elapsed\n",
      "Epoch 10,   350/  454; acc:  75.03; ppl:   3.05; 6665 src tok/s; 6935 tgt tok/s;     44 s elapsed\n",
      "Epoch 10,   400/  454; acc:  75.20; ppl:   3.02; 6980 src tok/s; 7197 tgt tok/s;     50 s elapsed\n",
      "Epoch 10,   450/  454; acc:  75.63; ppl:   2.93; 6918 src tok/s; 7171 tgt tok/s;     56 s elapsed\n",
      "Train perplexity: 3.20984\n",
      "Train accuracy: 74.2278\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 6.44516\n",
      "Validation accuracy: 68.9088\n",
      "Decaying learning rate to 0.125\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch 11,    50/  454; acc:  73.89; ppl:   3.29; 6700 src tok/s; 6967 tgt tok/s;      6 s elapsed\n",
      "Epoch 11,   100/  454; acc:  74.66; ppl:   3.12; 6521 src tok/s; 6774 tgt tok/s;     13 s elapsed\n",
      "Epoch 11,   150/  454; acc:  75.12; ppl:   3.07; 6677 src tok/s; 6923 tgt tok/s;     19 s elapsed\n",
      "Epoch 11,   200/  454; acc:  75.82; ppl:   2.90; 6505 src tok/s; 6799 tgt tok/s;     25 s elapsed\n",
      "Epoch 11,   250/  454; acc:  76.15; ppl:   2.85; 6482 src tok/s; 6746 tgt tok/s;     32 s elapsed\n",
      "Epoch 11,   300/  454; acc:  75.63; ppl:   2.93; 6796 src tok/s; 7008 tgt tok/s;     38 s elapsed\n",
      "Epoch 11,   350/  454; acc:  76.59; ppl:   2.78; 6739 src tok/s; 7012 tgt tok/s;     44 s elapsed\n",
      "Epoch 11,   400/  454; acc:  76.90; ppl:   2.76; 6585 src tok/s; 6790 tgt tok/s;     51 s elapsed\n",
      "Epoch 11,   450/  454; acc:  77.32; ppl:   2.67; 6538 src tok/s; 6777 tgt tok/s;     57 s elapsed\n",
      "Train perplexity: 2.91754\n",
      "Train accuracy: 75.8335\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 6.27725\n",
      "Validation accuracy: 69.0294\n",
      "Decaying learning rate to 0.0625\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch 12,    50/  454; acc:  74.80; ppl:   3.12; 6504 src tok/s; 6763 tgt tok/s;      6 s elapsed\n",
      "Epoch 12,   100/  454; acc:  75.52; ppl:   2.97; 6634 src tok/s; 6892 tgt tok/s;     13 s elapsed\n",
      "Epoch 12,   150/  454; acc:  75.88; ppl:   2.91; 6521 src tok/s; 6761 tgt tok/s;     19 s elapsed\n",
      "Epoch 12,   200/  454; acc:  76.72; ppl:   2.78; 6932 src tok/s; 7245 tgt tok/s;     25 s elapsed\n",
      "Epoch 12,   250/  454; acc:  77.04; ppl:   2.70; 6487 src tok/s; 6751 tgt tok/s;     31 s elapsed\n",
      "Epoch 12,   300/  454; acc:  76.45; ppl:   2.79; 6679 src tok/s; 6888 tgt tok/s;     38 s elapsed\n",
      "Epoch 12,   350/  454; acc:  77.35; ppl:   2.64; 6456 src tok/s; 6717 tgt tok/s;     45 s elapsed\n",
      "Epoch 12,   400/  454; acc:  77.85; ppl:   2.64; 6675 src tok/s; 6883 tgt tok/s;     51 s elapsed\n",
      "Epoch 12,   450/  454; acc:  78.10; ppl:   2.54; 6399 src tok/s; 6632 tgt tok/s;     57 s elapsed\n",
      "Train perplexity: 2.77642\n",
      "Train accuracy: 76.685\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 6.2124\n",
      "Validation accuracy: 69.4338\n",
      "Decaying learning rate to 0.03125\n",
      "\n",
      "Loading train dataset from data/multi30k.atok.low.train.1.pt, number of examples: 29000\n",
      "Epoch 13,    50/  454; acc:  75.16; ppl:   3.03; 6554 src tok/s; 6816 tgt tok/s;      6 s elapsed\n",
      "Epoch 13,   100/  454; acc:  76.07; ppl:   2.90; 6362 src tok/s; 6609 tgt tok/s;     13 s elapsed\n",
      "Epoch 13,   150/  454; acc:  76.51; ppl:   2.83; 6635 src tok/s; 6880 tgt tok/s;     19 s elapsed\n",
      "Epoch 13,   200/  454; acc:  77.27; ppl:   2.70; 6881 src tok/s; 7192 tgt tok/s;     25 s elapsed\n",
      "Epoch 13,   250/  454; acc:  77.47; ppl:   2.64; 6656 src tok/s; 6927 tgt tok/s;     31 s elapsed\n",
      "Epoch 13,   300/  454; acc:  76.95; ppl:   2.72; 6607 src tok/s; 6813 tgt tok/s;     38 s elapsed\n",
      "Epoch 13,   350/  454; acc:  77.76; ppl:   2.59; 6359 src tok/s; 6616 tgt tok/s;     45 s elapsed\n",
      "Epoch 13,   400/  454; acc:  78.24; ppl:   2.57; 6782 src tok/s; 6993 tgt tok/s;     51 s elapsed\n",
      "Epoch 13,   450/  454; acc:  78.74; ppl:   2.48; 6805 src tok/s; 7054 tgt tok/s;     57 s elapsed\n",
      "Train perplexity: 2.70679\n",
      "Train accuracy: 77.1736\n",
      "Loading valid dataset from data/multi30k.atok.low.valid.1.pt, number of examples: 1014\n",
      "Validation perplexity: 6.19626\n",
      "Validation accuracy: 69.5828\n",
      "Decaying learning rate to 0.015625\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
